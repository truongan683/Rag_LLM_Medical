import os
import sys

# 1. T·∫Øt Streamlit file watcher ƒë·ªÉ tr√°nh inspect c√°c module PyTorch
os.environ["STREAMLIT_SERVER_ENABLE_FILE_WATCHER"] = "false"

# 2. Tr√™n Windows: ƒë·∫∑t event loop policy r√µ r√†ng
if sys.platform.startswith("win"):
    import asyncio
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# 3. Patch torch.classes ƒë·ªÉ __path__ lu√¥n l√† list r·ªóng
import torch
torch.classes.__path__ = []

# 4. Import c√°c th√†nh ph·∫ßn
import streamlit as st
from config import (
    MODEL_PATH, LLM_CONFIG, EMBEDDING_MODEL, RERANKER_MODEL, CHROMA_DB_PATH,
    OPENAI_API_KEY, GPT_EMBEDDING_MODEL
)
from llama_cpp import Llama
from sentence_transformers import SentenceTransformer, CrossEncoder
import chromadb
import openai
from modules.llm_utils import OpenAIChatWrapper
# G√°n API key n·∫øu d√πng OpenAI
openai.api_key = OPENAI_API_KEY

def init_models(llm_choice):
    if "models_loaded" not in st.session_state:
        st.session_state["client"] = chromadb.PersistentClient(path=CHROMA_DB_PATH)

        if llm_choice == "Llama (Local)":
            st.session_state["llm_type"] = "llama"
            st.session_state["llm"] = Llama(model_path=MODEL_PATH, **LLM_CONFIG)
        elif llm_choice == "GPT (API)":
            st.session_state["llm_type"] = "openai"
            st.session_state["llm"] = OpenAIChatWrapper(model="gpt-4.1-mini")

        st.session_state["embedder"] = SentenceTransformer(EMBEDDING_MODEL)
        st.session_state["reranker"] = CrossEncoder(RERANKER_MODEL)
        st.session_state["models_loaded"] = True

# Giao di·ªán StreamGPT
st.set_page_config(page_title="Chatbot Y t·∫ø", layout="wide")
st.title("üß† H·ªá th·ªëng Chatbot Y t·∫ø")

llm_choice = st.selectbox("Ch·ªçn LLM", ["Llama (Local)", "GPT (API)"])

if st.button("üöÄ Kh·ªüi t·∫°o m√¥ h√¨nh"):
    init_models(llm_choice)
    st.success(f"ƒê√£ kh·ªüi t·∫°o m√¥ h√¨nh v·ªõi: {llm_choice}")
